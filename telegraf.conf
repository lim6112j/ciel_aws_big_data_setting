# Telegraf Configuration for InfluxDB to PostgreSQL Data Pipeline

# Global tags can be specified here in key="value" format.
[global_tags]
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.  This controls the size of writes that
  ## Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at
  ## the cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Log at debug level.
  # debug = false
  ## Log only error level messages.
  # quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  # logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0d"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/New_York
  # log_with_timezone = ""

  ## Override default hostname, if empty use os.Hostname()
  hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  omit_hostname = false

###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################

# Configuration for sending metrics to PostgreSQL
[[outputs.postgresql]]
  ## Specify connection string
  ## Format: postgres://[pqgotest[:password]]@localhost[/dbname]?sslmode=[disable|require|verify-ca|verify-full]
  ## Or a URL string:
  ## Format: host=localhost user=pqgotest password=... dbname=app_production sslmode=disable
  ## All connection parameters are optional.
  ##
  ## Without the dbname parameter, the driver will default to a database
  ## with the same name as the user. This dbname is just for instantiating a
  ## connection with the server and doesn't restrict the databases we are trying
  ## to connect to.
  connection = "postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}?sslmode=disable"

  ## Postgres schema, defaults to 'public'
  schema = "public"

  ## Store tags as foreign keys in the metrics table. Default is false.
  tags_as_foreign_keys = false

  ## Suffix to append to table name (measurement name) for the foreign key table.
  tag_table_suffix = "_tag"

  ## Deny inserting metrics if the foreign key table does not exist for the tag.
  foreign_key_constraint = false

  ## Store all tags as a JSONB object in a single 'tags' column.
  tags_as_jsonb = true

  ## Store all fields as a JSONB object in a single 'fields' column.
  fields_as_jsonb = true

  ## Templated table name to use.
  ## Available template variables are:
  ##   {TABLE} - tablename as identifier
  ##   {TABLELITERAL} - tablename as string literal
  ##   {COLUMNS} - column definitions
  ##   {KEY_COLUMNS} - comma-separated list of key columns (time + tags)
  # table_template = "CREATE TABLE {TABLE}({COLUMNS})"

  ## Template to use for creating tables.
  ## Available template variables are:
  ##   {TABLE} - tablename as identifier
  ##   {TABLELITERAL} - tablename as string literal
  ##   {COLUMNS} - column definitions
  ##   {KEY_COLUMNS} - comma-separated list of key columns (time + tags)
  # create_templates = [
  #   '''CREATE TABLE {TABLE}({COLUMNS})''',
  # ]

  ## Template to use for adding columns.
  ## Available template variables are:
  ##   {TABLE} - tablename as identifier
  ##   {TABLELITERAL} - tablename as string literal
  ##   {COLUMN} - column definition
  # add_column_template = "ALTER TABLE {TABLE} ADD COLUMN IF NOT EXISTS {COLUMN}"

  ## Timeout for all CockroachDB operations.
  # timeout = "5s"

###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################

# Read metrics from InfluxDB v2
[[inputs.influxdb_v2_listener]]
  ## Address and port to host InfluxDB listener on
  service_address = ":8087"

  ## Maximum duration before timing out read of the request
  read_timeout = "10s"
  ## Maximum duration before timing out write of the response
  write_timeout = "10s"

  ## Maximum allowed http request body size in bytes.
  ## 0 means to use the default of 32MiB.
  max_body_size = "32MiB"

  ## Optional tag name used to store the database name.
  ## If the write has a database in the query string then it will be kept in this tag name.
  ## This tag can be used in downstream outputs.
  ## The default value of nothing means it will be off and the database will not be recorded.
  # database_tag = ""

  ## If set the bucket tag will be added to every metric with the bucket used.
  ## The default value of nothing means it will be off and the bucket will not be recorded.
  # bucket_tag = ""

# Read metrics from InfluxDB v2 using queries
[[inputs.influxdb_v2]]
  ## The URLs of the InfluxDB cluster nodes.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  ##   ex: urls = ["https://us-west-2-1.aws.cloud2.influxdata.com"]
  urls = ["${INFLUXDB_URL}"]

  ## Token for authentication.
  token = "${INFLUXDB_TOKEN}"

  ## Organization is the name of the organization you wish to write to; must exist.
  organization = "${INFLUXDB_ORG}"

  ## Destination bucket to read from.
  bucket = "${INFLUXDB_BUCKET}"

  ## The value of this tag will be used to determine the bucket.  If this
  ## tag is not set the 'bucket' option is used as the default.
  # bucket_tag = ""

  ## Query to execute
  query = '''
  from(bucket: "${INFLUXDB_BUCKET}")
    |> range(start: -1h)
    |> filter(fn: (r) => r._measurement != "")
  '''

  ## Query language, either "flux" or "influxql"
  query_type = "flux"

  ## HTTP Timeout
  timeout = "20s"

  ## HTTP User-Agent
  # user_agent = "telegraf"

  ## Optional TLS Config for use on HTTP connections.
  # tls_ca = "/etc/telegraf/ca.pem"
  # tls_cert = "/etc/telegraf/cert.pem"
  # tls_key = "/etc/telegraf/key.pem"
  ## Use TLS but skip chain & host verification
  insecure_skip_verify = true

###############################################################################
#                            PROCESSOR PLUGINS                               #
###############################################################################

# Apply metric modifications using override semantics.
[[processors.override]]
  ## All modifications on inputs and aggregators can be overridden:
  # name_override = "new_name"
  # name_prefix = "new_name_prefix"
  # name_suffix = "new_name_suffix"

  ## Tags to be added (all values must be strings)
  [processors.override.tags]
    source = "influxdb"
    pipeline = "telegraf"

###############################################################################
#                            AGGREGATOR PLUGINS                              #
###############################################################################

# Keep the aggregate basicstats of each metric passing through.
[[aggregators.basicstats]]
  ## The period on which to flush & clear the aggregator.
  period = "30s"

  ## If true, the original metric will be dropped by the
  ## aggregator and will not get sent to the output plugins.
  drop_original = false

  ## Configures which basic stats to push as fields
  # stats = ["count", "min", "max", "mean", "stdev", "s2", "sum"]
